{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Lib Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    },
    "polyglot_notebook": {
     "kernelName": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "#load \"load.fsx\"\n",
    "\n",
    "open Newtonsoft.Json\n",
    "\n",
    "open Informedica.Ollama.Lib\n",
    "open Ollama.Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hello world of LLMs\n",
    "\n",
    "The operators contain the basic functions to initiate a conversation with a model and start asking questions. \n",
    "\n",
    "- The init function sets the system role and the model to run.\n",
    "- The `>>?` operator is used to ask a question and return a conversation\n",
    "- Using the `Ollama.Converstation.print` function you can print out the complete conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    },
    "polyglot_notebook": {
     "kernelName": "fsharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with gemma\n",
      "\n",
      "Options:\n",
      "{\"num_keep\":null,\"seed\":101,\"num_predict\":null,\"top_k\":null,\"top_p\":null,\"tfs_z\":null,\"typical_p\":null,\"repeat_last_n\":64,\"temperature\":0.0,\"repeat_penalty\":null,\"presence_penalty\":null,\"frequency_penalty\":null,\"mirostat\":0,\"mirostat_tau\":null,\"mirostat_eta\":null,\"penalize_newline\":null,\"stop\":[],\"numa\":null,\"num_ctx\":2048,\"num_batch\":null,\"num_gqa\":null,\"num_gpu\":null,\"main_gpu\":null,\"low_vram\":null,\"f16_kv\":null,\"vocab_only\":null,\"use_mmap\":null,\"use_mlock\":null,\"rope_frequency_base\":null,\"rope_frequency_scale\":null,\"num_thread\":null}\n",
      "\n",
      "Got an answer\n",
      "\n",
      "## Question:\n",
      "You are a helpful assistant\n",
      "\n",
      "## Answer:\n",
      "Sure, I am here to help you. Please tell me what you need me to do today. I am a powerful language model with a vast knowledge base and I am able to assist you with a wide range of tasks.\n",
      "\n",
      "Here are some of the things I can help you with:\n",
      "\n",
      "* **Information retrieval:** I can provide you with information on a wide range of topics.\n",
      "* **Language translation:** I can translate text between multiple languages.\n",
      "* **Code generation:** I can generate code in various programming languages.\n",
      "* **Question answering:** I can answer your questions in a comprehensive way.\n",
      "* **Conversation:** I can engage in conversation on a variety of topics.\n",
      "* **Creative writing:** I can help you write stories, poems, and other creative content.\n",
      "\n",
      "Please let me know what you need me to do and I will do my best to help you.\n",
      "\n",
      "\n",
      "\n",
      "## Question:\n",
      "Why is the sky blue?\n",
      "\n",
      "## Answer:\n",
      "Sure, here is why the sky is blue:\n",
      "\n",
      "The sky appears blue due to a phenomenon called **Rayleigh Scattering**.\n",
      "\n",
      "Here's the explanation:\n",
      "\n",
      "1. **Sunlight:** Sunlight consists of all the colors of the rainbow, including blue, red, green, and yellow.\n",
      "2. **Scattering:** When sunlight enters the Earth's atmosphere, particles of air scatter the different colors of the spectrum.\n",
      "3. **Blue Scatter:** The particles of air scatter the blue light more effectively than other colors because of their smaller size and the way they interact with light.\n",
      "4. **Scattered Light:** The scattered light, which includes a significant amount of blue light, is scattered in all directions.\n",
      "5. **Our Perception:** Our eyes perceive the scattered light as the color of the sky.\n",
      "\n",
      "This scattering of light is most noticeable when the sun is high in the sky, which is why the sky appears blue during the day. It is also why we sometimes see a blue tint in the air around sunset and sunrise, as the sun's rays have to travel farther through the atmosphere to reach our eyes.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"You are a helpful assistant\"\n",
    "|> init Ollama.Models.gemma\n",
    ">>? \"Why is the sky blue?\"\n",
    "|> Ollama.Conversation.print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the options for a model\n",
    "\n",
    "All the regular options can be set the will determine the behavior af a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    },
    "polyglot_notebook": {
     "kernelName": "fsharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with gemma\n",
      "\n",
      "Options:\n",
      "{\"num_keep\":null,\"seed\":101,\"num_predict\":null,\"top_k\":10,\"top_p\":0.95,\"tfs_z\":null,\"typical_p\":null,\"repeat_last_n\":64,\"temperature\":0.0,\"repeat_penalty\":null,\"presence_penalty\":null,\"frequency_penalty\":null,\"mirostat\":0,\"mirostat_tau\":null,\"mirostat_eta\":null,\"penalize_newline\":true,\"stop\":[],\"numa\":null,\"num_ctx\":2048,\"num_batch\":null,\"num_gqa\":null,\"num_gpu\":null,\"main_gpu\":null,\"low_vram\":null,\"f16_kv\":null,\"vocab_only\":null,\"use_mmap\":null,\"use_mlock\":null,\"rope_frequency_base\":null,\"rope_frequency_scale\":null,\"num_thread\":null}\n",
      "\n",
      "Got an answer\n",
      "\n",
      "## Question:\n",
      "You are a LLM specialist that can answer questions about LLMs\n",
      "\n",
      "## Answer:\n",
      "**Sure, I am an LLM specialist who can answer questions about LLMs:**\n",
      "\n",
      "**What is an LLM (Language Large Language Model)?**\n",
      "\n",
      "A Language Large Language Model (LLM) is a type of deep learning model that is trained on vast amounts of text data, enabling it to engage in natural language processing (NLP) tasks such as text generation, translation, and code synthesis. LLMs are a type of transformer-based model, which are known for their ability to capture long-range dependencies between words in a text.\n",
      "\n",
      "**Key Features of LLMs:**\n",
      "\n",
      "* **Text Generation:** LLMs can generate high-quality text in various formats, including articles, stories, code, and conversations.\n",
      "* **Translation:** LLMs can translate text between multiple languages with high accuracy.\n",
      "* **Question Answering:** LLMs can answer a wide range of questions based on text, code, or other information sources.\n",
      "* **Code Generation:** LLMs can generate code in multiple programming languages.\n",
      "* **Information Retrieval:** LLMs can retrieve relevant information from vast amounts of text data.\n",
      "\n",
      "**Types of LLMs:**\n",
      "\n",
      "* **Text-Only LLMs:** Trained on text data only, they excel in text generation, translation, and question answering.\n",
      "* **Multimodal LLMs:** Trained on text and other modalities (e.g., images, videos), they can handle tasks that require multiple modalities.\n",
      "* **Prompt-Based LLMs:** LLMs that are fine-tuned on specific prompts, enabling them to perform a wide range of tasks.\n",
      "\n",
      "**Applications of LLMs:**\n",
      "\n",
      "* Text summarization\n",
      "* Code translation\n",
      "* Code generation\n",
      "* Question answering\n",
      "* Chatbots\n",
      "* Content creation\n",
      "\n",
      "**Challenges Associated with LLMs:**\n",
      "\n",
      "* **Bias:** LLMs can inherit biases from the training data, which can lead to unfair or discriminatory outcomes.\n",
      "* **Ethical Concerns:** LLMs raise ethical concerns about privacy, misinformation, and potential misuse.\n",
      "* **Data Requirements:** LLMs require vast amounts of data for training, which can be computationally expensive.\n",
      "\n",
      "**Future Outlook:**\n",
      "\n",
      "LLMs are a rapidly evolving field with the potential to revolutionize many industries. As they continue to learn from vast amounts of data, LLMs are expected to become even more powerful and versatile.\n",
      "\n",
      "\n",
      "\n",
      "## Question:\n",
      "What do the following options do: \n",
      "- temperature\n",
      "- seed\n",
      "- penalize_newline\n",
      "- top_k\n",
      "- top_p\n",
      "\n",
      "## Answer:\n",
      "Sure, here's what each option does:\n",
      "\n",
      "**1. Temperature:**\n",
      "- Controls the randomness of the sampling process.\n",
      "- Higher temperature leads to more sampling from the probability distribution, resulting in more diverse and creative outputs.\n",
      "- Lower temperature leads to less sampling and more conformity to the training data, resulting in more accurate but less creative outputs.\n",
      "\n",
      "**2. Seed:**\n",
      "- Specifies a random number seed for the sampling process.\n",
      "- Different seeds will generate different samples from the same probability distribution.\n",
      "- Using a seed ensures reproducibility of results for a given model and input.\n",
      "\n",
      "**3. penalize_newline:**\n",
      "- Whether to penalize newlines in the generated text.\n",
      "- If True, newlines are penalized, making the model more likely to generate text without newlines.\n",
      "- If False, newlines are not penalized, allowing the model to generate text with newlines as needed.\n",
      "\n",
      "**4. top_k:**\n",
      "- Specifies the number of samples to generate from the probability distribution.\n",
      "- Higher values of top_k will generate more samples, increasing the likelihood of finding the best sample.\n",
      "- Lower values of top_k will generate fewer samples, making it more likely to find the best sample quickly.\n",
      "\n",
      "**5. top_p:**\n",
      "- Specifies the probability threshold for selecting samples from the probability distribution.\n",
      "- Samples with probabilities below this threshold are discarded, increasing the likelihood of generating samples with high probabilities.\n",
      "- Setting top_p to 1 will generate samples with probabilities greater than or equal to the highest probability in the distribution.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ollama.options.temperature <- 0\n",
    "Ollama.options.seed <- 101\n",
    "Ollama.options.penalize_newline <- true\n",
    "Ollama.options.top_k <- 10\n",
    "Ollama.options.top_p <- 0.95\n",
    "\n",
    "\"You are a LLM specialist that can answer questions about LLMs\"\n",
    "|> init Ollama.Models.gemma\n",
    ">>? \"\"\"\n",
    "What do the following options do: \n",
    "- temperature\n",
    "- seed\n",
    "- penalize_newline\n",
    "- top_k\n",
    "- top_p\n",
    "\"\"\"\n",
    "|> Ollama.Conversation.print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    },
    "polyglot_notebook": {
     "kernelName": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "(Ollama.listModels ()).models\n",
    "|> List.map (_.name)\n",
    "|> List.iter (printfn \"%s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     },
     {
      "aliases": [],
      "languageName": "fsharp",
      "name": "fsharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
