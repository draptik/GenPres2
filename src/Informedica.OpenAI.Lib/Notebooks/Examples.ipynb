{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Lib Examples\n",
    "\n",
    "First setup the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    },
    "polyglot_notebook": {
     "kernelName": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "#load \"load.fsx\"\n",
    "\n",
    "open Newtonsoft.Json\n",
    "\n",
    "open Informedica.OpenAI.Lib\n",
    "open Ollama.Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hello world of LLMs\n",
    "\n",
    "The operators contain the basic functions to initiate a conversation with a model and start asking questions. \n",
    "\n",
    "- The init function sets the system role and the model to run.\n",
    "- The `>>?` operator is used to ask a question and return a conversation\n",
    "- Using the `Ollama.Converstation.print` function you can print out the complete conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    },
    "polyglot_notebook": {
     "kernelName": "fsharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with gemma\n",
      "\n",
      "Options:\n",
      "{\"num_keep\":null,\"seed\":101,\"num_predict\":null,\"top_k\":null,\"top_p\":null,\"tfs_z\":null,\"typical_p\":null,\"repeat_last_n\":64,\"temperature\":0.0,\"repeat_penalty\":null,\"presence_penalty\":null,\"frequency_penalty\":null,\"mirostat\":0,\"mirostat_tau\":null,\"mirostat_eta\":null,\"penalize_newline\":null,\"stop\":[],\"numa\":null,\"num_ctx\":2048,\"num_batch\":null,\"num_gqa\":null,\"num_gpu\":null,\"main_gpu\":null,\"low_vram\":null,\"f16_kv\":null,\"vocab_only\":null,\"use_mmap\":null,\"use_mlock\":null,\"rope_frequency_base\":null,\"rope_frequency_scale\":null,\"num_thread\":null}\n",
      "\n",
      "ℹ INFO: \n",
      "EndPoint: http://localhost:11434/api/chat\n",
      "Payload:\n",
      "{\"messages\":[{\"content\":\"You are a helpful assistant\",\"role\":\"system\"},{\"content\":\"Why is the sky blue?\",\"role\":\"user\"}],\"model\":\"gemma\",\"options\":{\"num_keep\":null,\"seed\":101,\"num_predict\":null,\"top_k\":null,\"top_p\":null,\"tfs_z\":null,\"typical_p\":null,\"repeat_last_n\":64,\"temperature\":0.0,\"repeat_penalty\":null,\"presence_penalty\":null,\"frequency_penalty\":null,\"mirostat\":0,\"mirostat_tau\":null,\"mirostat_eta\":null,\"penalize_newline\":null,\"stop\":[],\"numa\":null,\"num_ctx\":2048,\"num_batch\":null,\"num_gqa\":null,\"num_gpu\":null,\"main_gpu\":null,\"low_vram\":null,\"f16_kv\":null,\"vocab_only\":null,\"use_mmap\":null,\"use_mlock\":null,\"rope_frequency_base\":null,\"rope_frequency_scale\":null,\"num_thread\":null},\"stream\":false}\n",
      "\n",
      "\n",
      "## System:\n",
      "You are a helpful assistant\n",
      "\n",
      "\n",
      "## Question:\n",
      "Why is the sky blue?\n",
      "\n",
      "## Answer:\n",
      "Sure, here is why the sky is blue:\n",
      "\n",
      "The sky appears blue due to a phenomenon called **Rayleigh Scattering**.\n",
      "\n",
      "Here's the explanation:\n",
      "\n",
      "1. **Sunlight:** Sunlight consists of all the colors of the rainbow, including blue, red, green, and yellow.\n",
      "2. **Scattering:** When sunlight enters the Earth's atmosphere, particles of air scatter the different colors of the spectrum.\n",
      "3. **Blue Scatter:** The particles of air scatter the blue light more effectively than other colors because of their smaller size and the way they interact with light.\n",
      "4. **Scattered Light:** The scattered light, which includes a significant amount of blue light, is scattered in all directions.\n",
      "5. **Our Perception:** Our eyes perceive the scattered light as the color of the sky.\n",
      "\n",
      "This scattering of light is most noticeable when the sun is high in the sky, which is why the sky appears blue during the day. It is also why we sometimes see a blue tint in the air around sunset and sunrise, as the sun's rays have to travel farther through the atmosphere to reach our eyes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"You are a helpful assistant\"\n",
    "|> init Ollama.Models.gemma\n",
    ">>? \"Why is the sky blue?\"\n",
    "|> Conversation.print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the options for a model\n",
    "\n",
    "All the regular options can be set the will determine the behavior af a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    },
    "polyglot_notebook": {
     "kernelName": "fsharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with gemma\n",
      "\n",
      "Options:\n",
      "{\"num_keep\":null,\"seed\":101,\"num_predict\":null,\"top_k\":10,\"top_p\":0.95,\"tfs_z\":null,\"typical_p\":null,\"repeat_last_n\":64,\"temperature\":0.0,\"repeat_penalty\":null,\"presence_penalty\":null,\"frequency_penalty\":null,\"mirostat\":0,\"mirostat_tau\":null,\"mirostat_eta\":null,\"penalize_newline\":true,\"stop\":[],\"numa\":null,\"num_ctx\":2048,\"num_batch\":null,\"num_gqa\":null,\"num_gpu\":null,\"main_gpu\":null,\"low_vram\":null,\"f16_kv\":null,\"vocab_only\":null,\"use_mmap\":null,\"use_mlock\":null,\"rope_frequency_base\":null,\"rope_frequency_scale\":null,\"num_thread\":null}\n",
      "\n",
      "ℹ INFO: \n",
      "EndPoint: http://localhost:11434/api/chat\n",
      "Payload:\n",
      "{\"messages\":[{\"content\":\"You are a LLM specialist that can answer questions about LLMs\",\"role\":\"system\"},{\"content\":\"\\nWhat do the following options do: \\n- temperature\\n- seed\\n- penalize_newline\\n- top_k\\n- top_p\\n\",\"role\":\"user\"}],\"model\":\"gemma\",\"options\":{\"num_keep\":null,\"seed\":101,\"num_predict\":null,\"top_k\":10,\"top_p\":0.95,\"tfs_z\":null,\"typical_p\":null,\"repeat_last_n\":64,\"temperature\":0.0,\"repeat_penalty\":null,\"presence_penalty\":null,\"frequency_penalty\":null,\"mirostat\":0,\"mirostat_tau\":null,\"mirostat_eta\":null,\"penalize_newline\":true,\"stop\":[],\"numa\":null,\"num_ctx\":2048,\"num_batch\":null,\"num_gqa\":null,\"num_gpu\":null,\"main_gpu\":null,\"low_vram\":null,\"f16_kv\":null,\"vocab_only\":null,\"use_mmap\":null,\"use_mlock\":null,\"rope_frequency_base\":null,\"rope_frequency_scale\":null,\"num_thread\":null},\"stream\":false}\n",
      "\n",
      "\n",
      "## System:\n",
      "You are a LLM specialist that can answer questions about LLMs\n",
      "\n",
      "\n",
      "## Question:\n",
      "What do the following options do: \n",
      "- temperature\n",
      "- seed\n",
      "- penalize_newline\n",
      "- top_k\n",
      "- top_p\n",
      "\n",
      "## Answer:\n",
      "Sure, here's what each option does:\n",
      "\n",
      "**1. Temperature:**\n",
      "- Controls the randomness of the sampling process.\n",
      "- Higher temperature leads to more sampling from the probability distribution, resulting in more diverse and creative outputs.\n",
      "- Lower temperature leads to less sampling and more conformity to the training data, resulting in more accurate but less creative outputs.\n",
      "\n",
      "**2. Seed:**\n",
      "- Specifies a random number seed for the sampling process.\n",
      "- Different seeds will generate different samples from the same probability distribution.\n",
      "- Using a seed ensures reproducibility of results for a given model and input.\n",
      "\n",
      "**3. penalize_newline:**\n",
      "- Whether to penalize newlines in the generated text.\n",
      "- If True, newlines are penalized, making the model more likely to generate text without newlines.\n",
      "- If False, newlines are not penalized, allowing the model to generate text with newlines as needed.\n",
      "\n",
      "**4. top_k:**\n",
      "- Specifies the number of samples to generate from the probability distribution.\n",
      "- Higher values of top_k will generate more samples, increasing the likelihood of finding the best sample.\n",
      "- Lower values of top_k will generate fewer samples, making it more likely to find the best sample quickly.\n",
      "\n",
      "**5. top_p:**\n",
      "- Specifies the probability threshold for selecting samples from the probability distribution.\n",
      "- Samples with probabilities below this threshold are discarded, increasing the likelihood of generating samples with high probabilities.\n",
      "- Setting top_p to 1 will generate samples with probabilities greater than or equal to the highest probability in the distribution.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ollama.options.temperature <- 0\n",
    "Ollama.options.seed <- 101\n",
    "Ollama.options.penalize_newline <- true\n",
    "Ollama.options.top_k <- 10\n",
    "Ollama.options.top_p <- 0.95\n",
    "\n",
    "\"You are a LLM specialist that can answer questions about LLMs\"\n",
    "|> init Ollama.Models.gemma\n",
    ">>? \"\"\"\n",
    "What do the following options do: \n",
    "- temperature\n",
    "- seed\n",
    "- penalize_newline\n",
    "- top_k\n",
    "- top_p\n",
    "\"\"\"\n",
    "|> Conversation.print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "fsharp"
    },
    "polyglot_notebook": {
     "kernelName": "fsharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ INFO: \n",
      "EndPoint: http://localhost:11434/api/tags\n",
      "\n",
      "joefamous/firefunction-v1:q3_k\n",
      "gemma:7b-instruct\n",
      "gemma:latest\n",
      "llama2:13b-chat\n",
      "llama2:latest\n",
      "meditron:latest\n",
      "medllama2:latest\n",
      "mistral:7b-instruct\n",
      "mistral:latest\n",
      "openchat:7b\n"
     ]
    }
   ],
   "source": [
    "Ollama.listModels ()\n",
    "|> Async.RunSynchronously\n",
    "|> function \n",
    "| Ok m -> \n",
    "    m.Response.models \n",
    "    |> List.map (_.name)\n",
    "    |> List.iter (printfn \"%s\")\n",
    "| Error err -> ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     },
     {
      "aliases": [],
      "languageName": "fsharp",
      "name": "fsharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
